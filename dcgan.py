#!/usr/bin/env python # coding: utf-8 # In[2]: from __future__ import absolute_import, division, print_function, unicode_literals import pathlib import tensorflow as tf tf.enable_eager_execution() tf.__version__ AUTOTUNE = tf.data.experimental.AUTOTUNE # In[3]: data_root_orig = r'/Users/iankonradjohnson/ArtGenerator/Datasets/test' data_root = pathlib.Path(data_root_orig) # In[4]: for item in data_root.iterdir(): print(item) # In[5]: import random all_image_paths = list(data_root.glob('*.jpg')) # In[6]: all_image_paths = [str(path) for path in all_image_paths] random.shuffle(all_image_paths) image_count = len(all_image_paths) # In[7]: import pandas as pd import ntpath data = pd.read_csv(data_root/"all_data_info.csv") filenames = data['new_filename'] filenames = [int(file[:-4]) for file in filenames] data.insert(1, 'filename', filenames, True) data = data.sort_values(by='filename') # In[8]: all_image_filenames = [ntpath.basename(file)[:-4] for file in all_image_paths] all_image_filenames = sorted(map(int, all_image_filenames)) # In[9]: import IPython.display as display def caption_image(image_path): filename = os.path.basename(image_path)[:-4] file = data.loc[data['filename'] == int(filename)] return str([file['title'] + "(" + file['date'] + ") by " + file['artist']])[9:-15] # In[ ]: import os for n in range(3): image_path = random.choice(all_image_paths) display.display(display.Image(image_path)) print(caption_image(image_path)) print() # In[ ]: label_names = sorted(list(set(data['style'].values.astype(str)))) label_names # In[ ]: label_to_index = dict((name, index) for index,name in enumerate(label_names)) label_to_index # In[ ]: all_image_labels = [] for index, row in data.iterrows(): if row['filename'] in all_image_filenames: print(row['style'], row['filename'], label_to_index[str(row['style'])]) all_image_labels.append(label_to_index[str(row['style'])]) # file = data.loc[data['style'] == label] # print("First 10 labels indices: ", all_image_labels[:10]) # In[ ]: all_image_labels # data # # Load and format the images # TensorFlow includes all the tools you need to load and process images: # In[ ]: img_path = all_image_paths[0] img_path # In[ ]: img_raw = tf.read_file(img_path) print(repr(img_raw)[:100]+"...") # In[ ]: img_tensor = tf.image.decode_image(img_raw) print(img_tensor.shape) print(img_tensor.dtype) # In[ ]: img_final = tf.image.resize(img_tensor, [192, 192]) img_final = img_final/255.0 print(img_final.shape) print(img_final.numpy().min()) print(img_final.numpy().max()) # In[ ]: def preprocess_image(image): image = tf.image.decode_jpeg(image, channels=3) image = tf.image.resize_image_with_pad(image, 240, 240) image /= 255.0 # normalize to [0,1] range return image # In[ ]: def load_and_preprocess_image(path): image = tf.read_file(path) return preprocess_image(image) # In[ ]: import matplotlib.pyplot as plt img_path = all_image_paths[2] label = all_image_labels[2] plt.imshow(load_and_preprocess_image(img_path)) plt.grid(False) plt.xlabel(caption_image(img_path).encode('utf-8')) plt.title(label_names[label].title()) print() # In[ ]: avg_height = sum(list(data['pixelsy'].values))/103250 avg_width = sum(list(data['pixelsx'].values))/103250 print(avg_height, avg_width) # # Build a tf.data.Dataset (A dataset of images) # # The easiest way to build a tf.data.Dataset is using the from_tensor_slices method. # # Slicing the array of strings results in a dataset of strings: # # In[ ]: path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths) path_ds # The output_shapes and output_types fields describe the content of each item in the dataset. In this case it is a set of scalar binary-strings # In[ ]: print('shape: ', repr(path_ds.output_shapes)) print('type: ', path_ds.output_types) print() print(path_ds) # Now create a new dataset that loads and formats images on the fly by mapping preprocess_image over the dataset of paths. # In[ ]: image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE) # In[ ]: import matplotlib.pyplot as plt plt.figure(figsize=(8,8)) for n,image in enumerate(image_ds.take(4)): plt.subplot(2,2,n+1) plt.imshow(image) plt.grid(False) plt.xticks([]) plt.yticks([]) plt.xlabel(caption_image(all_image_paths[n])) plt.show() # # A dataset of (image, label) pairs # Using the same from_tensor_slices method you can build a dataset of labels # In[ ]: label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64)) # In[ ]: for label in label_ds.take(10): print(label_names[label.numpy()]) # Since the datasets are in the same order you can just zip them together to get a dataset of (image, label) pairs. # In[ ]: image_label_ds = tf.data.Dataset.zip((image_ds, label_ds)) # The new dataset's shapes and types are tuples of shapes and types as well, describing each field: # In[ ]: print(image_label_ds) # In[ ]: ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels)) # The tuples are unpacked into the positional arguments of the mapped function def load_and_preprocess_from_path_label(path, label): return load_and_preprocess_image(path), label image_label_ds = ds.map(load_and_preprocess_from_path_label) image_label_ds # In[ ]: import glob import imageio import matplotlib.pyplot as plt import numpy as np import os import PIL from tensorflow.keras import layers import time # In[ ]: import matplotlib.pyplot as plt img_path = all_image_paths[3] label = all_image_labels[3] image = load_and_preprocess_image(img_path) plt.imshow(image) plt.grid(False) plt.xlabel(caption_image(img_path).encode('utf-8')) plt.title(label_names[label].title()) print() # In[ ]: train_images_path = "/Users/iankonradjohnson/ArtGenerator/Datasets/test/test_images.txt" train_images = [] train_images = [line.rstrip('\n') for line in open(train_images_path)] # train_images # In[ ]: prepared_images = [] for img_path in train_images: try: image = load_and_preprocess_image(img_path) except Exception as e: continue print(image) prepared_images.append(image) # In[ ]: train_labels # In[ ]: prepared_images = np.asarray(train_images) prepared_images # In[ ]: # train_images = prepared_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') train_images # In[ ]: BUFFER_SIZE = 6000 BATCH_SIZE = 256 # In[ ]: # Batch and shuffle the data train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) train_dataset # # Create the models # # Both the generator and discriminator are defined using the Keras Sequential API. # # # The Generator # # The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh. # In[ ]: def make_generator_model(): model = tf.keras.Sequential() model.add(layers.Dense(60*60*256, use_bias=False, input_shape=(100,))) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Reshape((60, 60, 256))) assert model.output_shape == (None, 60, 60, 256) # Note: None is the batch size model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)) assert model.output_shape == (None, 60, 60, 128) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)) assert model.output_shape == (None, 120, 120, 64) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')) assert model.output_shape == (None, 240, 240, 3) return model # Use the (as yet untrained) generator to create an image. # In[ ]: from PIL import Image generator = make_generator_model() noise = tf.random.normal([1, 100]) generated_image = generator(noise, training=False) generated_image # In[ ]: plt.imshow(generated_image[0, :, :, 0]) generated_image[0, :, :, 0] # generated_image[1, :, :, 0] # # The Discriminator # # The discriminator is a CNN-based image classifier. # In[ ]: def make_discriminator_model(): model = tf.keras.Sequential() model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[240, 240, 3], data_format="channels_last")) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Flatten()) model.add(layers.Dense(3)) return model # Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images. # In[ ]: discriminator = make_discriminator_model() decision = discriminator(generated_image) print (decision) # # Define the loss and optimizers # # Define loss functions and optimizers for both models. # In[ ]: # This method returns a helper function to compute cross entropy loss cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) # # Discriminator loss # # This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s. # In[ ]: def discriminator_loss(real_output, fake_output): real_loss = cross_entropy(tf.ones_like(real_output), real_output) fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) total_loss = real_loss + fake_loss return total_loss # # Generator loss # # The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s. # # # In[ ]: def generator_loss(fake_output): return cross_entropy(tf.ones_like(fake_output), fake_output) # The discriminator and the generator optimizers are different since we will train two networks separately. # In[ ]: generator_optimizer = tf.keras.optimizers.Adam(1e-4) discriminator_optimizer = tf.keras.optimizers.Adam(1e-4) # # Define the training loop # In[ ]: EPOCHS = 50 noise_dim = 100 num_examples_to_generate = 16 # We will reuse this seed overtime (so it's easier) # to visualize progress in the animated GIF) seed = tf.random.normal([num_examples_to_generate, noise_dim]) # The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator. # In[ ]: # Notice the use of `tf.function` # This annotation causes the function to be "compiled". # @tf.function def train_step(images): noise = tf.random.normal([BATCH_SIZE, noise_dim]) with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: generated_images = generator(noise, training=True) # real_output = discriminator(images, training=True) fake_output = discriminator(generated_images, training=True) gen_loss = generator_loss(fake_output) disc_loss = discriminator_loss(real_output, fake_output) gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables) gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) # In[ ]: def train(dataset, epochs): for epoch in range(epochs): start = time.time() for image_batch in dataset: train_step(image_batch) # Produce images for the GIF as we go display.clear_output(wait=True) generate_and_save_images(generator, epoch + 1, seed) # Save the model every 15 epochs if (epoch + 1) % 15 == 0: checkpoint.save(file_prefix = checkpoint_prefix) print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start)) # Generate after the final epoch display.clear_output(wait=True) generate_and_save_images(generator, epochs, seed) # # Generate and save images # In[ ]: def generate_and_save_images(model, epoch, test_input): # Notice `training` is set to False. # This is so all layers run in inference mode (batchnorm). predictions = model(test_input, training=False) fig = plt.figure(figsize=(4,4)) for i in range(predictions.shape[0]): plt.subplot(4, 4, i+1) plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray') plt.axis('off') plt.savefig('image_at_epoch_{:04d}.png'.format(epoch)) plt.show() # # Train the model # # Call the train() method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate). # # At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab. # # In[ ]: # train(train_dataset, EPOCHS) train(image_ds, EPOCHS) # for image_batch in image_ds: # print(image_batch) # In[ ]: # In[ ]: